{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8aSqxq-lbwwu",
        "QfvGF-ftcLHQ",
        "8m_g4OxtWitv",
        "RpDYds3tceST"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP3"
      ],
      "metadata": {
        "id": "bR7d2AVJbqim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back on TP2"
      ],
      "metadata": {
        "id": "8aSqxq-lbwwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "vToZQaP3bsoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to solve $Ax = b$\n",
        "with\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "   2 & -1 & 0 & 0 & \\cdots & 0 & 0\\\\\n",
        "   -1 & 2 & -1 & 0 &\\cdots & 0 & 0 \\\\\n",
        "   0 & -1 & 2 & -1 &\\cdots & 0 & 0 \\\\\n",
        "   0 & 0 & -1 & 2 &\\cdots & 0 & 0 \\\\\n",
        "   \\vdots  & \\vdots & \\vdots & \\vdots& \\ddots & \\vdots & \\vdots  \\\\\n",
        "   0 & 0 & 0 & 0 & \\cdots & 2 & -1 \\\\\n",
        "   0 & 0 & 0 & 0 & \\cdots & -1 & 2 \n",
        " \\end{pmatrix} \\in \\mathbb{R}^{n \\times n} = \\mathscr{M}_n(\\mathbb{R})\n",
        " \\text{ and } \n",
        " b = \n",
        " \\begin{pmatrix}\n",
        " 1 \\\\\n",
        " 1 \\\\\n",
        " \\vdots \\\\\n",
        " 1 \\\\\n",
        " 1\n",
        " \\end{pmatrix} \\in \\mathbb{R}^n\n",
        "$$"
      ],
      "metadata": {
        "id": "yJUuvwRbtdW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is sufficient to minimize the functional :       \n",
        "$$F : x \\in \\mathbb{R}^n \\mapsto \\frac{1}{2}\\langle Ax,x \\rangle - \\langle b, x \\rangle$$\n",
        "\n",
        "1. Implement the matrix ```A``` and the vector ```b``` in ```torch``` with $n = 20$\n",
        "2. Compute the gradient of F w.r.t a random vector ```x``` with ```torch.autograd```\n",
        "3. Verify that the computation of the gradient is correct. .\n",
        "3. Implement it using ```torch.optim.SGD```, a learning rate equal to 0.1, a momentum equal to 0.9 and ```10**3``` iterations."
      ],
      "metadata": {
        "id": "lZSqeyhlwgnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.\n",
        "n=20\n",
        "A = 2*torch.eye(n)-torch.diag(torch.ones(n-1),1)-torch.diag(torch.ones(n-1),-1)\n",
        "b = torch.ones(n)"
      ],
      "metadata": {
        "id": "-pmFIkHLnVEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "x = torch.randn(n,requires_grad = True)\n",
        "F = 1/2*(torch.matmul(A,x)*x).sum() - (b*x).sum()\n",
        "F.backward() #In 1D, equivalent to F.backward(gradient=torch.tensor(1.))"
      ],
      "metadata": {
        "id": "j_Wntu8SyUXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "print(torch.norm(x.grad-torch.matmul(A,x)+b))"
      ],
      "metadata": {
        "id": "lQipJf0XztR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "x = torch.randn(n,requires_grad = True)\n",
        "optim = torch.optim.SGD([x], lr=1e-1,momentum=0.9)\n",
        "Nit = 10**3\n",
        "for k in range(Nit) :\n",
        "  optim.zero_grad() #x.grad *= 0\n",
        "  F = 1/2*(torch.matmul(A,x)*x).sum() - (b*x).sum()\n",
        "  F.backward()\n",
        "  optim.step() #x = x-lr*x.grad"
      ],
      "metadata": {
        "id": "STJ0hxZ42EuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.norm(torch.matmul(A,x)-b))"
      ],
      "metadata": {
        "id": "utiW3DsVQBFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neural Networks\n"
      ],
      "metadata": {
        "id": "Ijv3_YISBvY9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfvGF-ftcLHQ"
      },
      "source": [
        "\n",
        "Neural networks can be constructed using the ``torch.nn`` package.\n",
        "\n",
        "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
        "``autograd`` to define models and differentiate them.\n",
        "An ``nn.Module`` contains layers, and a method ``forward(input)`` that\n",
        "returns the ``output``.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "## Define the network\n",
        "\n",
        "Let’s define a network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnjtDwjqcLHO"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtWlSylQcLHR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question** : Describe the architecture of the network. </font>"
      ],
      "metadata": {
        "id": "qUM6LLlcTid4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question** : Print the weights and the bias of the first convolution </font>"
      ],
      "metadata": {
        "id": "k79XwF0AbNyr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kfzsfEgcLHS"
      },
      "source": [
        "You just have to define the ``forward`` function, and the ``backward``\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using ``autograd``.\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "The learnable parameters of a model are returned by ``net.parameters()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sie6Ow3cLHS"
      },
      "outputs": [],
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Be careful !** When we speak about the number of parameters of a network, we speack about the number of real numbers in the matrices, biases and kernels involved in the network."
      ],
      "metadata": {
        "id": "G8QrCj4NV3LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question** : How many parameters are in this network ? </font>"
      ],
      "metadata": {
        "id": "mRU32WSbWWdV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWC9IfM8cLHS"
      },
      "source": [
        "Let's try a random 32x32 input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question** : Test the network on a random input which is a grayscale image (one channel) of size $32 \\times 32$. </font>"
      ],
      "metadata": {
        "id": "xXZPV8UzbjRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBBFwR0jcLHS"
      },
      "outputs": [],
      "source": [
        "input = #TODO\n",
        "out = net(input)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dzyQLfCcLHT"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
        "    package only supports inputs that are a mini-batch of samples, and not\n",
        "    a single sample.\n",
        "\n",
        "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
        "    ``nSamples x nChannels x Height x Width``.\n",
        "\n",
        "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
        "    a fake batch dimension.</p></div>\n",
        "\n",
        "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "\n",
        "**Recap:**\n",
        "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
        "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
        "     tensor.\n",
        "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
        "     encapsulating parameters*, with helpers for moving them to GPU,\n",
        "     exporting, loading, etc.\n",
        "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
        "     registered as a parameter when assigned as an attribute to a*\n",
        "     ``Module``.\n",
        "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
        "     of an autograd operation*. Every ``Tensor`` operation creates at\n",
        "     least a single ``Function`` node that connects to functions that\n",
        "     created a ``Tensor`` and *encodes its history*.\n",
        "\n",
        "**At this point, we covered:**\n",
        "  -  Defining a neural network\n",
        "  -  Processing inputs\n",
        "\n",
        "**Still Left:**\n",
        "  -  Calling backwad\n",
        "  -  Computing the loss\n",
        "  -  Updating the weights of the network\n",
        "\n",
        "## Loss Function\n",
        "A loss function takes the (output, target) pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different\n",
        "[loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the\n",
        "nn package .\n",
        "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
        "between the output and the target.\n",
        "\n",
        "For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0quxyS30cLHT"
      },
      "outputs": [],
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DUX6lFVcLHU"
      },
      "source": [
        "Now, if you follow ``loss`` in the backward direction, using its\n",
        "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
        "like this:\n",
        "\n",
        "\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "          -> loss\n",
        "\n",
        "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
        "w.r.t. the neural net parameters, and all Tensors in the graph that have\n",
        "``requires_grad=True`` will have their ``.grad`` Tensor accumulated with the\n",
        "gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWct2PdccLHU"
      },
      "source": [
        "## Backprop\n",
        "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
        "gradients before and after the backward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMuThSVmcLHV"
      },
      "outputs": [],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color='blue'> Exercise </font>"
      ],
      "metadata": {
        "id": "8m_g4OxtWitv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We would like to define a network that classifies images of size $(3,32,32)$.\n",
        "Construct a class Net() which applies : \n",
        "- a convolution with $6$ channels out and a kernel of size $5 \\times 5$, followed by a ReLu\n",
        "- a max pooling of size $2 \\times 2$\n",
        "- a convolution with $16$ channels out and a kernel of size $5 \\times 5$, followed by a ReLu\n",
        "- a linear layer with an output size $120$, followed by a ReLu\n",
        "- a linear layer with an output size $84$, followed by a ReLu\n",
        "- a linear layer with an output size $10$\n",
        "\n",
        "Verify that it works with a random input.\n",
        "\n",
        "2. Draw the network as it is done in the last slides on the course."
      ],
      "metadata": {
        "id": "MO-VJ2GtWyRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "      #TODO\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "saejSg-qcatJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = #TODO\n",
        "net(x)"
      ],
      "metadata": {
        "id": "LHa2zo3gZGZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='blue'> For your information : images with tensor </font>"
      ],
      "metadata": {
        "id": "C8Tn9948rPEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Quand on ouvre une image avec torch, elle est au format \"pil\"\n",
        "- On peut la convertir en tensor, elle sera de shape [3,M,N], à valeurs dans [0,1]\n",
        "- Pour convertir un tensor en format pil, il faut d'abord appliquer un *.clip(0,1)* pour le rendre à valeurs dans [0,1]\n",
        "- Pour l'afficher, il faut la re-convertir en format pil et utiliser *display*\n",
        "- Pour l'enregistrer, il faut la re-convertir en format pil et utiliser *.save('nom.png')*\n",
        "- Pour afficher une image en tensor avec plt.imshow, il faut la convertir en numpy et la transformer en shape [M,N,3]\n",
        "- L'avantage de display est que cela rend compte de la vraie taille de l'image. *plt.imshow()* effectue des interpolations cachées."
      ],
      "metadata": {
        "id": "OYLy_httt7ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "#Open an image\n",
        "os.system(\"wget -c  https://render.fineartamerica.com/images/rendered/default/framed-print/images-medium-5/simeon-denis-poisson-french-physicist-science-source.jpg?imgWI=7.5&imgHI=10&sku=CRQ13&mat1=PM918&mat2=&t=2&b=2&l=2&r=2&off=0.5&frameW=0.875\")\n",
        "img_pil = Image.open(\"simeon-denis-poisson-french-physicist-science-source.jpg?imgWI=7.5\")\n",
        "\n",
        "#Afficher l'image\n",
        "img_as_tensor = to_tensor(img_pil)\n",
        "print(img_as_tensor.shape)\n",
        "img_as_tensor = img_as_tensor.clip(0,1) #Le clip sert à assurer que les valeurs de l'images soient comprises entre 0 et 1 ( évite un warning de Python )\n",
        "pil_img = to_pil_image(img_as_tensor) #Ceci convertit le tensor au Format PIL\n",
        "display(pil_img)\n",
        "\n",
        "#Enregistrer l'image\n",
        "#pil_img.save('Simeon.png')\n",
        "\n",
        "#si on voulait utiliser plt.imshow\n",
        "import pylab as plt \n",
        "import numpy as np\n",
        "npimg = img_as_tensor.numpy()\n",
        "print(np.transpose(npimg, (1, 2, 0)).shape)\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KWD6u7LfrMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color='blue'> Exercise (eventually done during the next TP) </font>"
      ],
      "metadata": {
        "id": "RpDYds3tceST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to the TP1"
      ],
      "metadata": {
        "id": "na08ssA5ci8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs, make_gaussian_quantiles\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "QivrfWQDZpAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class = 3\n",
        "# Three examples of synthetic 2D datasets:\n",
        "X, t = make_blobs(n_features=2, centers = n_class,n_samples=100) \n",
        "X, t = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=24, n_classes=n_class, n_clusters_per_class=1,n_samples=200)\n",
        "X, t = make_gaussian_quantiles(n_features=2, n_classes=n_class, n_samples=500)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=.4, random_state=12)\n",
        "\n",
        "# Number of points in each set:\n",
        "N_train = X_train.shape[0]\n",
        "N_test = X_test.shape[0]\n",
        "\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k');\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YNraF_AIfmy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question :** Code a network composed of a linear layer with ouput size $d$ followed by a Relu and a final layer allowing the classification  </font>"
      ],
      "metadata": {
        "id": "87kysY3wp9ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "d = 4\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      #TODO\n",
        "\n",
        "    def forward(self, x):\n",
        "      #TODO\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "w-_nN9s1fow8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='blue'> **Question :** Complete the next cell </font>"
      ],
      "metadata": {
        "id": "ofd5vW75pck_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion =  #TODO\n",
        "optimizer = optim.SGD(______________, lr=0.001, momentum=0.9) #TODO\n",
        "\n",
        "#Conversion of the numpy in tensors\n",
        "X_train, X_test, t_train, t_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32), torch.tensor(t_train, dtype=torch.int64), torch.tensor(t_test, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "cKJVqjHIfqtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training of the network\n",
        "\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "\n",
        "    for i, x in enumerate(X_train, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        x = x.unsqueeze(0)\n",
        "        t = t_train[i].unsqueeze(0)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = net(x)\n",
        "        loss = criterion(output, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "LxNknh0Xf1zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize results:\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "h = 0.02\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "X_grid = np.hstack((xx.ravel(), yy.ravel()))\n",
        "\n",
        "N_grid = xx.ravel().shape[0]\n",
        "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "feature_transform = lambda x : (net(torch.tensor(x, dtype=torch.float32).unsqueeze(0)).detach().numpy())\n",
        "\n",
        "Phi_grid = feature_transform(X_grid)\n",
        "\n",
        "Z =np.argmax(Phi_grid,axis=2)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "figure = plt.figure(figsize=(16, 8))\n",
        "ax = plt.subplot(1,2,1)\n",
        "ax.set_title(\"Input data\")\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
        "ax = plt.subplot(1,2,2)\n",
        "cmap = ListedColormap(['b','y','r','m','g','c'])\n",
        "plt.contourf(xx,yy,Z,  cmap = cmap, alpha=.8)\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')"
      ],
      "metadata": {
        "id": "BZFCv6RLh9N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bgm-odvsjluc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}